{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega 1 - jugador de Teeko\n",
    "\n",
    "### Grupo 7:\n",
    "     - S. Agustina Sierra Lima C.I. 4.647.235-6\n",
    "     - V. Sebastian Volti Diano C.I...\n",
    "     - C. Alejandro Clara C.I. 4.772.294-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta tarea es construir un jugador de Teeko utilizando t√©cnicas de aprendizaje autom√°tico.\n",
    "En el marco de la definici√≥n de aprendizaje autom√°tico identificamos las tres variables $T$, $P$ y $E$ en nuestro dise√±o de la siguiente manera:\n",
    "\n",
    "- Deseamos mejorar en la tarea $T$: Jugar al Teeko.\n",
    "\n",
    "- Respecto a la siguiente medida $P$: % de juegos ganados.\n",
    "\n",
    "- Bas√°ndonos en la experiencia $E$:\n",
    "\n",
    "  i) Inicialmente el algoritmo aprende jugando contra un oponente que juega de forma aleatoria.\n",
    "  \n",
    "  ii) Luego se usa como oponente una versi√≥n aprendida en alguna iteraci√≥n anterior. \n",
    "\n",
    "Para representar lo que efectivamente queremos aprender hemos definido ciertas funciones:   \n",
    "  \n",
    "$ChooseMove:tablero\\rightarrow movimiento$, la cual toma un tablero como entrada y su salida es un movimiento del conjunto de movimientos posibles que un jugador puede hacer.\n",
    "\n",
    "$V:tablero\\rightarrow \\mathbb{R}$, donde $V$ asigna los valores mas altos a los mejores tableros, es decir, a los que tienen mayores chances de llevar a ganar la partida.\n",
    "\n",
    "De esta forma hemos traducido el problema de mejorar el rendimiento $P$ de una tarea $T$, al problema de encontrar la funci√≥n $ChooseMove$ tal que, dado un tablero devuelve el mejor movimiento posible para un jugador. Es decir, el movimiento que lleve al tablero con valor m√°s alto seg√∫n $V$, dentro de los movimientos posibles.\n",
    "\n",
    "Tal y como vimos en clase la funci√≥n $V$ no es efectivamente computable, lo que la hace no operativa.\n",
    "El objetivo que nos proponemos es encontrar la mejor aproximaci√≥n a tal funci√≥n.  \n",
    "Entonces definimos:  \n",
    "  \n",
    "$V_{op}:tablero\\rightarrow \\mathbb{R}$\n",
    "Donde $V_{op}$ la definimos como una serie de caracter√≠sticas que representan el estado del tablero y adem√°s cumple que:  \n",
    "  \n",
    "$V_{op}(t_{ganador})=1$  \n",
    "$V_{op}(t_{perdedor})=-1$  \n",
    "$V_{op}(t_{empate})=0$\n",
    "\n",
    "\n",
    "$V_{op}(t)=w_{0}+w_{1}X_{1}+w_{2}X_{2}+w_{3}X_{3}+w_{4}X_{4}+w_{5}X_{5}+w_{6}X_{6}+w_{7}X_{7}+w_{8}X_{8}$  \n",
    "  \n",
    "Donde:    \n",
    "$X_{1}=$ Cantidad de adyacencias horizontales del jugador 1   \n",
    "$X_{2}=$ Cantidad de adyacencias vertivales del jugador 1  \n",
    "$X_{3}=$ Cantidad de adyacencias diagonales vistas de izquierda (arriba) a derecha (abajo) del jugador 1   \n",
    "$X_{4}=$ Cantidad de adyacencias diagonales vistas de derecha (arriba) a izquierda (abajo) del jugador 1   \n",
    "$X_{5}=$ Cantidad de adyacencias horizontales del jugador 2   \n",
    "$X_{6}=$ Cantidad de adyacencias verticales del jugador 2  \n",
    "$X_{7}=$ Cantidad de adyacencias diagonales vistas de izquierda (arriba) a derecha (abajo) del jugador 2  \n",
    "$X_{8}=$ Cantidad de adyacencias diagonales vistas de derecha (arriba) a izquierda (abajo) del jugador 2   \n",
    "  \n",
    "En conclusi√≥n, nos proponemos encontrar los valores $w_{i}$ √≥ptimos para aproximar $V_{op}$ a $V$.\n",
    "Utilizamos la t√©cnica de minimos cuadrados vista en clase, variando algunos par√°metros que veremos m√°s adelante en el informe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dise√±o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Juego\n",
    "\n",
    "- El juego se compone de un tablero de 5x5 y cuatro fichas para cada jugador. El objetivo es lograr tener cuatro fichas en l√≠nea recta (vertical, horizontal o diagonal) o en un cuadrado de cuatro espacios adyacentes.\n",
    "- Una de las simplificaciones especificadas en la letra es que se comienza con un tablero con todas sus fichas posicionadas y solo resta moverlas. Para elegir tales tableros iniciales utilizamos una funci√≥n que los genera aleatoriamente.\n",
    "- Existen fichas negras y rojas, dado que las reglas indican que siempre comienza el jugador con las fichas negras diremos que el jugador que nos interesa entrenar (el jugador 1) es el que juega primero, es decir, el que tiene las fichas negras. Las fichas rojas corresponder√°n al oponente (el jugador 2).\n",
    "- Definimos como empate si se dan mas de 500 movimientos en total y a√∫n ning√∫n jugador ha ganado.\n",
    "\n",
    "\n",
    "## 2.2 Tablero\n",
    "\n",
    "- Representamos un tablero como una matriz de 5x5 en donde colocamos 1's en las coordenadas que el jugador 1 tiene sus fichas y 2's en las coordenadas donde el jugador 2 tiene sus fichas. El resto de los lugares libres se representan con 0's.\n",
    "- Luego de haber le√≠do el ejemplo del juego de damas quisimos trazar una similitud con el juego en cuesti√≥n. Creemos que las caracter√≠sticas del tablero que elegimos son las m√°s representativas del estado del juego, en el sentido de que con solo listarlas nos da una idea de qu√© jugador est√° m√°s cerca de ganar y asi poder llegar a una forma correcta de dar valores a los diferentes estados de los tableros. Tambi√©n decidimos elegir estos atributos porque son los que mejor representan las distintas formas de ganar que tienen los jugadores.\n",
    "  \n",
    "A continuaci√≥n, una serie de ejemplos:  \n",
    "  \n",
    "$\\begin{pmatrix}\n",
    "0 & 1 & 1 & 1 & 1 \\\\\n",
    "0 & 2 & 0 & 0 & 2 \\\\\n",
    "0 & 2 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 2 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}$ $X_{1}=$ 3, $X_{2}=$ 0, $X_{3}=$ 0, $X_{4}=$ 0, $X_{5}=$ 0, $X_{6}=$ 1, $X_{7}=$ 1, $X_{8}=$ 0  \n",
    "\n",
    "$\\begin{pmatrix}\n",
    "0 & 0 & 1 & 0 & 1 \\\\\n",
    "0 & 2 & 2 & 0 & 1 \\\\\n",
    "0 & 2 & 2 & 0 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}$  $X_{1}=$ 0, $X_{2}=$ 2, $X_{3}=$ 0, $X_{4}=$ 0, $X_{5}=$ 2, $X_{6}=$ 2, $X_{7}=$ 1, $X_{8}=$ 1\n",
    "\n",
    "$\\begin{pmatrix}  \n",
    "1 & 0 & 2 & 2 & 2 \\\\\n",
    "0 & 1 & 0 & 0 & 2 \\\\\n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}$ $X_{1}=$ 0, $X_{2}=$ 0, $X_{3}=$ 3, $X_{4}=$ 0, $X_{5}=$ 2, $X_{6}=$ 1, $X_{7}=$ 1, $X_{8}=$ 0  \n",
    "  \n",
    "Estos son ejemplos de tableros finales. En t√©rminos generales sabemos que tenemos tableros finales si:\n",
    "- Las adyacencias horizontales son 3 √≥\n",
    "- Las adyacencias verticales son 3 √≥\n",
    "- Las adyacencias diagonales de izquierda (arriba) a derecha (abajo) son 3 √≥\n",
    "- Las adyacencias diagonales de derecha (arriba) a izquierda (abajo) son 3 √≥\n",
    "- Las adyacencias horizontales son 2 y las adyacencias verticales son 2\n",
    "\n",
    "\n",
    "## 2.3 Algoritmo\n",
    "\n",
    "### 2.3.1 Visi√≥n general\n",
    "\n",
    "En t√©rminos generales, y siguiendo la idea planteada en el libro, dividimos el algoritmo en 4 m√≥dulos: _Performance System, Critic, Generalizer y Generator_.\n",
    "\n",
    "**Performance System**: Utilizando las funciones aprendidas, es el que se encarga de realizar una partida. Toma como entrada un tablero inicial y su salida corresponde a un historial de juego.\n",
    "Definimos un historial como todos los tableros por los que pasa el jugador 1 luego de que realiza un movimiento.  \n",
    "  \n",
    "**Critic**: Se encarga de producir los ejemplos de entrenamiento. Toma como entrada un historial de juego y su salida es un conjunto de tuplas $<t,V_{train}(t)>$, donde $V_{train}(t)$ es un valor aproximado que calculamos de la siguiente manera:\n",
    "  \n",
    "  $V_{train}(t_{intermedio}) ‚Üê V_{op}(t'_{siguiente\\space tablero\\space en\\space el\\space historial})$\n",
    "  \n",
    "  \n",
    "**Generalizer**: Toma como entrada los ejemplos de entrenamiento y su salida es una nueva aproximaci√≥n de la funci√≥n objetivo, luego de haber aplicado el algoritmo de minimos cuadrados. Aqu√≠ es donde se ajustan los valores $w_{i}$. El algoritmo de ajuste que utilizamos es el que se dio en clase.\n",
    "  \n",
    "**Generator**: Es el encargado tanto de generar un nuevo tablero inicial, as√≠ como de configurar todos los par√°metros necesarios para una nueva partida. Toma como par√°metro una funci√≥n $V_{op}$ y su salida es un nuevo tablero inicial.\n",
    "\n",
    "Definimos una **iteraci√≥n** como una vuelta completa sobre cada uno de los m√≥dulos definidos arriba.\n",
    "\n",
    "### 2.3.2 Ajuste de la funci√≥n\n",
    "\n",
    "El orden para completar un ajuste de la funci√≥n objetivo ser√≠a: _Generator -> Performance System -> Critic -> Generalizer_ y el ajuste se realiza en el m√≥dulo _Generalizer_.\n",
    "\n",
    "### 2.3.3 Parametrizaci√≥n\n",
    "\n",
    "A nivel de implementaci√≥n, decidimos dise√±ar el c√≥digo de manera tal que nos resultara f√°cil ajustar los par√°metros para el momento de realizar la experimentaci√≥n. En dicha secci√≥n se prueba con varios valores de Œº, en algunos casos se prueba con enfriamiento, se prueba con un factor de descuento, etc.\n",
    "\n",
    "## 2.4 Contrarios\n",
    "\n",
    "- Para generar los tableros al azar se genera una matriz de 5x5 con valores todos en 0. Luego, para poblar el tablero, para cada jugador se generan tuplas que representan las coordenadas de d√≥nde va a ser colocada la ficha. En ambas coordenadas se usa la funci√≥n de Python randint(), hasta que se encuentra un lugar libre y se coloca all√≠ la ficha.\n",
    "- Para generar los movimientos al azar se generan todos los posibles siguientes tableros, se los guarda en una lista _L_ y se usa la funci√≥n de Python choice sobre _L_, la cual retorna un elemento al azar.\n",
    "- Los $w_{i}$ se actualizan en cada iteraci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimentaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Oponente Aleatorio \n",
    "\n",
    "En esta primera parte mostraremos los resultados obtenidos de jugar con un oponente que siempre juegue de forma aleatoria.\n",
    "En cada punto se jugo un total de 120 partidas, ajustando los $w_{i}$ luego de cada partida.\n",
    "\n",
    "### 3.1.1 $w_{i}$ iniciales\n",
    "\n",
    "Siempre que se comienza a entrenar, se realiza un entrenamiento inicial con tableros finales valorados con 1, 0 o -1. Una vez generado este conjunto de tableros finales creamos un historial y llamamos a los modulos _Critic_ y luego a _Generalizer_. As√≠, ajustamos tantas veces como tableros finales haya en el conjunto. Finalmente, con la funci√≥n $V$ y con los  $w_{i}$ conseguidos iniciamos el entrenamiento.\n",
    "     \n",
    "Probamos con tres vectores iniciales (pre-entrenamiento inicial) distintos y nos quedamos con el vector de $w_{i}$ que nos dio un porcentaje m√°s alto de partidas ganadas.  \n",
    "Dichos vectores fueron los siguientes:  \n",
    "     1. (0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25)\n",
    "     2. (0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75)\n",
    "     3. (0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45)\n",
    "    \n",
    "  Resultados:  \n",
    "<table>\n",
    "  <tr style=\"font-weight:bold\">\n",
    "    <th>(w0,w1,w2,w3,w4,w5,w6,w7,w8)</th>\n",
    "    <th>Partidas ganadas</th>\n",
    "    <th>% de partidas ganadas</th>\n",
    "    <th>% de partidas empatadas</th>\n",
    "  </tr>   \n",
    "  <tr>\n",
    "    <td>(0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25)</td>\n",
    "    <td>91</td>\n",
    "    <td>50.5</td>\n",
    "    <td>13.3</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75)</td>\n",
    "    <td>76</td>\n",
    "    <td>42.2</td>\n",
    "    <td>15.6</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45)</td>\n",
    "    <td>106</td>\n",
    "    <td>58.8</td>\n",
    "    <td>13.8</td>\n",
    "  </tr>    \n",
    "</table>\n",
    "\n",
    "En conclusi√≥n, nos quedamos con el vector inicial $W$ igual a 0.45 en todas sus entradas.  \n",
    "  \n",
    "### 3.1.2 Elecci√≥n del proximo movimiento\n",
    "En todos los experimentos que realizamos en esta secci√≥n, el jugador dado el conjunto de todos los movimientos posbles elige el que tiene una valoracion mayor segun la funcion objetivo $V$ que conoce hasta el momento.  \n",
    "\n",
    "### 3.1.3 Aproximacion para $ùëâ_{ùë°ùëüùëéùëñùëõ}(ùë°)$\n",
    "\n",
    "Si bien al principio la idea fue mapear el espacio de los posibles tableros a un entorno (-1, 1) donde los tableros ganadores tomaran el valor 1 y los perdedores el valor -1, esto no se pudo lograr de forma estricta, dado que los $w_{i}$ tend√≠an a crecer y las caracteristicas que decidimos representar de los tableros pod√≠an llegar a ser n√∫meros que hicieran crecer la funci√≥n, haciendo que tomara valores m√°s grandes que 1.\n",
    "\n",
    "Adem√°s, de alguna manera, imponer que los tableros finales tuvieran valores -1 o 1 despistaba al algoritmo. Creemos que puede haber sido por tener tableros anteriores con valores m√°s (menos) grandes que un tablero final ganador (perdedor). Lo que hicimos para enfrentar este problema fue directamente descartar los tableros finales al momento de generar el conjunto de entrenamiento. \n",
    "\n",
    "Otra opci√≥n podr√≠a haber sido utilizar la noci√≥n de que dado un $t\\space/\\space t\\space es \\space final =>$ $V_{train}(t)=V_{op}(t)$ logrando as√≠ mantener la idea de que los tableros con valores mas grandes son mejores.\n",
    "\n",
    "### 3.1.4 Movimientos aleatorios\n",
    "Uno de los fen√≥menos que observamos es que luego de muchos entrenamientos el algoritmo se estancaba y no exporaba nuevos tableros. Decidimos probar tomando cada X movimientos un movimiento aleatorio y ver qu√© suced√≠a con el porcentaje de partidas gandas. Este fue el resultado:\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Sin movimientos aleatorios</th>\n",
    "    <th>Cada 2 movimientos uno aleatorio</th>\n",
    "    <th>Cada 3 movimientos uno aleatorio</th>\n",
    "    <th>Cada 4 movimientos uno aleatorio</th>\n",
    "    <th>Cada 5 movimientos uno aleatorio</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>%</td>\n",
    "    <td>77.7%</td>\n",
    "    <td>86.6%</td>\n",
    "    <td>80.5%</td>\n",
    "    <td>83.3%</td>\n",
    "  </tr>    \n",
    "</table>\n",
    "  \n",
    "  En conclusi√≥n, nos quedaremos con la configuraci√≥n que nos dio un total de 86.6% de partidas ganadas. Es decir, elegir un movimiento aleatorio cada 3 movimientos.\n",
    "  \n",
    "  _**Observaci√≥n:** Notamos que luego de fijar esta variante casi no tenemos empates._\n",
    "  \n",
    "### 3.1.5 Tasa de aprendizaje Œº y enfriamiento\n",
    "A nivel conceptual, a mayor valor de Œº, mayor es el ajuste. Fuimos probando con distintos valores de Œº en cada entrenamiento, y los resultados fueron los siguientes:\n",
    "    \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Œº</th>\n",
    "    <th>% partidas ganadas</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Œº=0.1</td>\n",
    "    <td>93.3</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>Œº=0.01</td>\n",
    "    <td>94.4</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>Œº=0.001</td>\n",
    "    <td>95.6</td>\n",
    "  </tr>  \n",
    "</table>\n",
    "\n",
    "En un intento por mejorar a√∫n m√°s el porcentaje probamos con distintas tasas de enfriamiento, pero no notamos cambios notorios en los resultados finales.\n",
    "\n",
    "En conclusi√≥n, nos quedarnos con Œº fijo igual a 0.001.\n",
    "\n",
    "\n",
    "### 3.1.6 Factor de descuento $\\gamma$\n",
    "Definimos un factor de descuento igual a $(0.9)^k$ con el objetivo de dar una menor valoraci√≥n a los tableros que estan m√°s cerca del tablero inicial.\n",
    "\n",
    "El porcentaje de partidas ganadas sin el factor de descuento fue de 95.6%.\n",
    "\n",
    "Con el factor de descuento fue de 96.4%, por lo que decidimos conservarlo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Oponente inteligente\n",
    "\n",
    "En esta secci√≥n veremos los resultados obtenidos al jugar contra un oponente que es una version anterior del jugador 1.\n",
    "\n",
    "### 3.2.1 Oponente con pesos fijos\n",
    " \n",
    "Primero probamos entrenar contra un jugador 2 que utiliza la funcion objetivo $V_{op}$ parte 3.1 y nucna la actualiza :\n",
    "\n",
    "Aqu√≠ notamos que con los hiperparamentros utilizados en la parte 3.2 el jugador 1 tenia un porcentaje de victorias mucho menor, alredodr de un 40% lo que nos llevo a cambiar variables.  \n",
    "- Inicialmente aumentamos la cantidad de partidas jugadas.  \n",
    "- Luego cambiamos la cantidad de movimientos aleatorios del jugador 1 a cero, pues en la parte 3.2 ya exploro             nuevos caminos y esta vez queremos que tome siempre el movimiento optimo con la funcion objetivo conocida         hasta el momento.  \n",
    "- Como notamos que igualmente el porcentaje de partidas ganadas era bajo, decidimos probar con distintos factores de aprendizaje.\n",
    "- Otro fenomeno que notamos fue el aumento de empates que en la seccion anterior habiamos logrado bajar.  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Œº = 0.001</th>\n",
    "    <th>Œº = 0.01</th>\n",
    "    <th>Œº = 0.1</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>jugador 1 23.4% / juegador 2 22.7% </td>\n",
    "    <td>jugador 1 23.9% / juegador 2 21.8%</td>\n",
    "    <td>jugador 1 49.6% / juegador 2 41.0%</td>  \n",
    "  </tr>\n",
    "     <caption>Esta tabla representa el porcentaje de partidas ganadas de cada jugador  jugador 1/jugador 2 variando Œº </caption>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Aqui podemos ver tres graficas donde cada una representa la suma de la cantidad de partidas ganadas para cada jugador en funcion de la cantidad de patidas jugadas. La orimera es con Œº=0.001 , la segunda con Œº=0.01 y la tercera Œº=0.1  \n",
    "  \n",
    "<img src=\"grafica1.jpeg\" width=400 height=400>  \n",
    "\n",
    "\n",
    "<img src=\"grafica2.jpeg\" width=400 height=400>  \n",
    "  \n",
    "\n",
    "<img src=\"grafica3.jpeg\" width=400 height=400>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Oponente actualizado\n",
    "\n",
    "Luego acualizamos la funcion $V_{op}$ que ultiliza el jugador 2 para elegir su proximo movimiento cada 20 partidas:\n",
    "\n",
    "Aqui el porcentaje de partidas ganadas por el jugador 1 es y si vemos la grafica analoga a la parte anterior se ve claramente como el jugador 2 gana mas partidads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Competencia\n",
    "\n",
    "En este punto pusimos a competir dos versiones iguales del jugador 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8d88052198>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot\n",
    "matplotlib.pyplot.plot(range(0,100), [2**-(x/10) for x in range(0,100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusi√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Los mejores resultados logicamente se dieron cuendo jugabamos con un opnente aleatorio , dado que la inteligencia de tal era nula. A medida que el oponente se volvia mas inteligente el porcentaje de partidas ganadas decrecio notoriamente. En este punto tambien observamos que con un oponete aleatorio casi no habian empates.\n",
    "- Otro punto a destacar, cuando oponente es aleatorio el echo de que usar enfriamiento en la taza de aprendizaje casi no cambio los resultados, con un Œº =0.001 que es bastante peque√±o obtuvimos buenos resultados. Lo que significa que si desde un principio limitabamos la variacion de el vector $W$ podiamos converger a valores satisfactorios. En cotraposicion con esto cuando el oponente es una version anterior del jugador 1 necitamos una taza de aprendizaje inicial mas alta. Obtuvimos mejores resultados cambiando Œº =0.1 y el enfriamiento en este punto tampoco tuvo grandes variaciones en el resultado final.\n",
    "- Tambien notamos que cuando entrenamos contra un oponente random, en un momento el porcentaje de partidas ganadas se estanca. Esto pudimos solucionarlo haciendo que el jugador 1 cada cierta cantidad de movimientos realize uno aleatorio, permitiendo asi que explore nuevos caminos y no se estanque su aprendizaje. \n",
    "  Lo curioso de esto es que cuando entrenamos contra un oponente inteligente tomar movimintos aleatorios no cambia el porcentaje de partidas ganadas. Atribuimos esto a que el la version anteror ya exploro todos los caminos que puede con los hiperparametros que conocidos.  \n",
    "- Algo que creemos que mejoraria el algoritmo es poder normalizar los atributos y los $w_{i}$ de forma tal de poder logara que la imagen de la funcion $V_{op}$ realemnte se encuentrte en el intervalos $[-1,1]$. Logrando asi poder darle valores conocidos a tableros finales. \n",
    "  Inicialmente creimos conveniente que para tener una idea de que valores $w_{i}$ para comenza a jugar podia ser    utiles, crear historiales de juego que conste de solo un tablero final dado que en principio $V_{train}$ para esos tableros era conocida , pero con el avance de la tare nos dimos cuenta que era una falacia dado que existian tableros intemedios con un valor de $V_{op}$ mayor que 1.  \n",
    "- Tambie podria mejorar el algoritmo cambiar la forma en la que elegimos el proximo movimiento, la forma que nosotros aplicamos si bien nos fue util no creemos que sea la mejor. Una alternativa a esto es dado el conjunto de movimientos posibles para el jugador 1 si suponemos que nuestro oponente se mueve de forma optima (segun la $V_{op}$ conocida hasta el momento) podemoms quedarnos con el movimiento que lleve a el oponente a el tablero menor valorado para el."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
