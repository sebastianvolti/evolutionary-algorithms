{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega 1 - jugador de Teeko\n",
    "\n",
    "### Grupo 7:\n",
    "     - S. Agustina Sierra Lima C.I. 4.647.235-6\n",
    "     - V. Sebastian Volti Diano C.I...\n",
    "     - C. Alejandro Clara C.I. 4.772.294-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta tarea es construir un jugador de Teeko utilizando técnicas de aprendizaje automático.\n",
    "En el marco de la definición de aprendizaje automático identificamos las tres variables $T$, $P$ y $E$ en nuestro diseño de la siguiente manera:\n",
    "\n",
    "- Deseamos mejorar en la tarea $T$: Jugar al Teeko.\n",
    "\n",
    "- Respecto a la siguiente medida $P$: % de juegos ganados.\n",
    "\n",
    "- Basándonos en la experiencia $E$:\n",
    "\n",
    "  i) Inicialmente el algoritmo aprende jugando contra un oponente que juega de forma aleatoria.\n",
    "  \n",
    "  ii) Luego se usa como oponente una versión aprendida en alguna iteración anterior. \n",
    "\n",
    "Para representar lo que efectivamente queremos aprender hemos definido ciertas funciones:   \n",
    "  \n",
    "$ChooseMove:tablero\\rightarrow movimiento$, la cual toma un tablero como entrada y su salida es un movimiento del conjunto de movimientos posibles que un jugador puede hacer.\n",
    "\n",
    "$V:tablero\\rightarrow \\mathbb{R}$, donde $V$ asigna los valores mas altos a los mejores tableros, es decir, a los que tienen mayores chances de llevar a ganar la partida.\n",
    "\n",
    "De esta forma hemos traducido el problema de mejorar el rendimiento $P$ de una tarea $T$, al problema de encontrar la función $ChooseMove$ tal que, dado un tablero devuelve el mejor movimiento posible para un jugador. Es decir, el movimiento que lleve al tablero con valor más alto según $V$, dentro de los movimientos posibles.\n",
    "\n",
    "Tal y como vimos en clase la función $V$ no es efectivamente computable, lo que la hace no operativa.\n",
    "El objetivo que nos proponemos es encontrar la mejor aproximación a tal función.  \n",
    "Entonces definimos:  \n",
    "  \n",
    "$V_{op}:tablero\\rightarrow \\mathbb{R}$\n",
    "Donde $V_{op}$ la definimos como una serie de características que representan el estado del tablero y además cumple que:  \n",
    "  \n",
    "$V_{op}(t_{ganador})=1$  \n",
    "$V_{op}(t_{perdedor})=-1$  \n",
    "$V_{op}(t_{empate})=0$\n",
    "\n",
    "\n",
    "$V_{op}(t)=w_{0}+w_{1}X_{1}+w_{2}X_{2}+w_{3}X_{3}+w_{4}X_{4}+w_{5}X_{5}+w_{6}X_{6}+w_{7}X_{7}+w_{8}X_{8}$  \n",
    "  \n",
    "Donde:    \n",
    "$X_{1}=$ Cantidad de adyacencias horizontales del jugador 1   \n",
    "$X_{2}=$ Cantidad de adyacencias vertivales del jugador 1  \n",
    "$X_{3}=$ Cantidad de adyacencias diagonales vistas de izquierda (arriba) a derecha (abajo) del jugador 1   \n",
    "$X_{4}=$ Cantidad de adyacencias diagonales vistas de derecha (arriba) a izquierda (abajo) del jugador 1   \n",
    "$X_{5}=$ Cantidad de adyacencias horizontales del jugador 2   \n",
    "$X_{6}=$ Cantidad de adyacencias verticales del jugador 2  \n",
    "$X_{7}=$ Cantidad de adyacencias diagonales vistas de izquierda (arriba) a derecha (abajo) del jugador 2  \n",
    "$X_{8}=$ Cantidad de adyacencias diagonales vistas de derecha (arriba) a izquierda (abajo) del jugador 2   \n",
    "  \n",
    "En conclusión, nos proponemos encontrar los valores $w_{i}$ óptimos para aproximar $V_{op}$ a $V$.\n",
    "Utilizamos la técnica de minimos cuadrados vista en clase, variando algunos parámetros que veremos más adelante en el informe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Diseño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Juego\n",
    "\n",
    "- El juego se compone de un tablero de 5x5 y cuatro fichas para cada jugador. El objetivo es lograr tener cuatro fichas en línea recta (vertical, horizontal o diagonal) o en un cuadrado de cuatro espacios adyacentes.\n",
    "- Una de las simplificaciones especificadas en la letra es que se comienza con un tablero con todas sus fichas posicionadas y solo resta moverlas. Para elegir tales tableros iniciales utilizamos una función que los genera aleatoriamente.\n",
    "- Existen fichas negras y rojas, dado que las reglas indican que siempre comienza el jugador con las fichas negras diremos que el jugador que nos interesa entrenar (el jugador 1) es el que juega primero, es decir, el que tiene las fichas negras. Las fichas rojas corresponderán al oponente (el jugador 2).\n",
    "- Definimos como empate si se dan mas de 500 movimientos en total y aún ningún jugador ha ganado.\n",
    "\n",
    "\n",
    "## 2.2 Tablero\n",
    "\n",
    "- Representamos un tablero como una matriz de 5x5 en donde colocamos 1's en las coordenadas que el jugador 1 tiene sus fichas y 2's en las coordenadas donde el jugador 2 tiene sus fichas. El resto de los lugares libres se representan con 0's.\n",
    "- Luego de haber leído el ejemplo del juego de damas quisimos trazar una similitud con el juego en cuestión. Creemos que las características del tablero que elegimos son las más representativas del estado del juego, en el sentido de que con solo listarlas nos da una idea de qué jugador está más cerca de ganar y asi poder llegar a una forma correcta de dar valores a los diferentes estados de los tableros. También decidimos elegir estos atributos porque son los que mejor representan las distintas formas de ganar que tienen los jugadores.\n",
    "  \n",
    "A continuación, una serie de ejemplos:  \n",
    "  \n",
    "$\\begin{pmatrix}\n",
    "0 & 1 & 1 & 1 & 1 \\\\\n",
    "0 & 2 & 0 & 0 & 2 \\\\\n",
    "0 & 2 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 2 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}$ $X_{1}=$ 3, $X_{2}=$ 0, $X_{3}=$ 0, $X_{4}=$ 0, $X_{5}=$ 0, $X_{6}=$ 1, $X_{7}=$ 1, $X_{8}=$ 0  \n",
    "\n",
    "$\\begin{pmatrix}\n",
    "0 & 0 & 1 & 0 & 1 \\\\\n",
    "0 & 2 & 2 & 0 & 1 \\\\\n",
    "0 & 2 & 2 & 0 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}$  $X_{1}=$ 0, $X_{2}=$ 2, $X_{3}=$ 0, $X_{4}=$ 0, $X_{5}=$ 2, $X_{6}=$ 2, $X_{7}=$ 1, $X_{8}=$ 1\n",
    "\n",
    "$\\begin{pmatrix}  \n",
    "1 & 0 & 2 & 2 & 2 \\\\\n",
    "0 & 1 & 0 & 0 & 2 \\\\\n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}$ $X_{1}=$ 0, $X_{2}=$ 0, $X_{3}=$ 3, $X_{4}=$ 0, $X_{5}=$ 2, $X_{6}=$ 1, $X_{7}=$ 1, $X_{8}=$ 0  \n",
    "  \n",
    "Estos son ejemplos de tableros finales. En términos generales sabemos que tenemos tableros finales si:\n",
    "- Las adyacencias horizontales son 3 ó\n",
    "- Las adyacencias verticales son 3 ó\n",
    "- Las adyacencias diagonales de izquierda (arriba) a derecha (abajo) son 3 ó\n",
    "- Las adyacencias diagonales de derecha (arriba) a izquierda (abajo) son 3 ó\n",
    "- Las adyacencias horizontales son 2 y las adyacencias verticales son 2\n",
    "\n",
    "\n",
    "## 2.3 Algoritmo\n",
    "\n",
    "### 2.3.1 Visión general\n",
    "\n",
    "En términos generales, y siguiendo la idea planteada en el libro, dividimos el algoritmo en 4 módulos: _Performance System, Critic, Generalizer y Generator_.\n",
    "\n",
    "**Performance System**: Utilizando las funciones aprendidas, es el que se encarga de realizar una partida. Toma como entrada un tablero inicial y su salida corresponde a un historial de juego.\n",
    "Definimos un historial como todos los tableros por los que pasa el jugador 1 luego de que realiza un movimiento.  \n",
    "  \n",
    "**Critic**: Se encarga de producir los ejemplos de entrenamiento. Toma como entrada un historial de juego y su salida es un conjunto de tuplas $<t,V_{train}(t)>$, donde $V_{train}(t)$ es un valor aproximado que calculamos de la siguiente manera:\n",
    "  \n",
    "  $V_{train}(t_{intermedio}) ← V_{op}(t'_{siguiente\\space tablero\\space en\\space el\\space historial})$\n",
    "  \n",
    "  \n",
    "**Generalizer**: Toma como entrada los ejemplos de entrenamiento y su salida es una nueva aproximación de la función objetivo, luego de haber aplicado el algoritmo de minimos cuadrados. Aquí es donde se ajustan los valores $w_{i}$. El algoritmo de ajuste que utilizamos es el que se dio en clase.\n",
    "  \n",
    "**Generator**: Es el encargado tanto de generar un nuevo tablero inicial, así como de configurar todos los parámetros necesarios para una nueva partida. Toma como parámetro una función $V_{op}$ y su salida es un nuevo tablero inicial.\n",
    "\n",
    "Definimos una **iteración** como una vuelta completa sobre cada uno de los módulos definidos arriba.\n",
    "\n",
    "### 2.3.2 Ajuste de la función\n",
    "\n",
    "El orden para completar un ajuste de la función objetivo sería: _Generator -> Performance System -> Critic -> Generalizer_ y el ajuste se realiza en el módulo _Generalizer_.\n",
    "\n",
    "### 2.3.3 Parametrización\n",
    "\n",
    "A nivel de implementación, decidimos diseñar el código de manera tal que nos resultara fácil ajustar los parámetros para el momento de realizar la experimentación. En dicha sección se prueba con varios valores de μ, en algunos casos se prueba con enfriamiento, se prueba con un factor de descuento, etc.\n",
    "\n",
    "## 2.4 Contrarios\n",
    "\n",
    "- Para generar los tableros al azar se genera una matriz de 5x5 con valores todos en 0. Luego, para poblar el tablero, para cada jugador se generan tuplas que representan las coordenadas de dónde va a ser colocada la ficha. En ambas coordenadas se usa la función de Python randint(), hasta que se encuentra un lugar libre y se coloca allí la ficha.\n",
    "- Para generar los movimientos al azar se generan todos los posibles siguientes tableros, se los guarda en una lista _L_ y se usa la función de Python choice sobre _L_, la cual retorna un elemento al azar.\n",
    "- Los $w_{i}$ se actualizan en cada iteración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimentación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Oponente Aleatorio \n",
    "\n",
    "En esta primera parte mostraremos los resultados obtenidos de jugar con un oponente que siempre juegue de forma aleatoria.\n",
    "En cada punto se jugo un total de 120 partidas, ajustando los $w_{i}$ luego de cada partida.\n",
    "\n",
    "### 3.1.1 $w_{i}$ iniciales\n",
    "\n",
    "Siempre que se comienza a entrenar, se realiza un entrenamiento inicial con tableros finales valorados con 1, 0 o -1. Una vez generado este conjunto de tableros finales creamos un historial y llamamos a los modulos _Critic_ y luego a _Generalizer_. Así, ajustamos tantas veces como tableros finales haya en el conjunto. Finalmente, con la función $V$ y con los  $w_{i}$ conseguidos iniciamos el entrenamiento.\n",
    "     \n",
    "Probamos con tres vectores iniciales (pre-entrenamiento inicial) distintos y nos quedamos con el vector de $w_{i}$ que nos dio un porcentaje más alto de partidas ganadas.  \n",
    "Dichos vectores fueron los siguientes:  \n",
    "     1. (0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25)\n",
    "     2. (0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75)\n",
    "     3. (0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45)\n",
    "    \n",
    "  Resultados:  \n",
    "<table>\n",
    "  <tr style=\"font-weight:bold\">\n",
    "    <th>(w0,w1,w2,w3,w4,w5,w6,w7,w8)</th>\n",
    "    <th>Partidas ganadas</th>\n",
    "    <th>% de partidas ganadas</th>\n",
    "    <th>% de partidas empatadas</th>\n",
    "  </tr>   \n",
    "  <tr>\n",
    "    <td>(0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25)</td>\n",
    "    <td>91</td>\n",
    "    <td>50.5</td>\n",
    "    <td>13.3</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75)</td>\n",
    "    <td>76</td>\n",
    "    <td>42.2</td>\n",
    "    <td>15.6</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45)</td>\n",
    "    <td>106</td>\n",
    "    <td>58.8</td>\n",
    "    <td>13.8</td>\n",
    "  </tr>    \n",
    "</table>\n",
    "\n",
    "En conclusión, nos quedamos con el vector inicial $W$ igual a 0.45 en todas sus entradas.  \n",
    "  \n",
    "### 3.1.2 Elección del proximo movimiento\n",
    "En todos los experimentos que realizamos en esta sección, el jugador dado el conjunto de todos los movimientos posbles elige el que tiene una valoracion mayor segun la funcion objetivo $V$ que conoce hasta el momento.  \n",
    "\n",
    "### 3.1.3 Aproximacion para $𝑉_{𝑡𝑟𝑎𝑖𝑛}(𝑡)$\n",
    "\n",
    "Si bien al principio la idea fue mapear el espacio de los posibles tableros a un entorno (-1, 1) donde los tableros ganadores tomaran el valor 1 y los perdedores el valor -1, esto no se pudo lograr de forma estricta, dado que los $w_{i}$ tendían a crecer y las caracteristicas que decidimos representar de los tableros podían llegar a ser números que hicieran crecer la función, haciendo que tomara valores más grandes que 1.\n",
    "\n",
    "Además, de alguna manera, imponer que los tableros finales tuvieran valores -1 o 1 despistaba al algoritmo. Creemos que puede haber sido por tener tableros anteriores con valores más (menos) grandes que un tablero final ganador (perdedor). Lo que hicimos para enfrentar este problema fue directamente descartar los tableros finales al momento de generar el conjunto de entrenamiento. \n",
    "\n",
    "Otra opción podría haber sido utilizar la noción de que dado un $t\\space/\\space t\\space es \\space final =>$ $V_{train}(t)=V_{op}(t)$ logrando así mantener la idea de que los tableros con valores mas grandes son mejores.\n",
    "\n",
    "### 3.1.4 Movimientos aleatorios\n",
    "Uno de los fenómenos que observamos es que luego de muchos entrenamientos el algoritmo se estancaba y no exporaba nuevos tableros. Decidimos probar tomando cada X movimientos un movimiento aleatorio y ver qué sucedía con el porcentaje de partidas gandas. Este fue el resultado:\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Sin movimientos aleatorios</th>\n",
    "    <th>Cada 2 movimientos uno aleatorio</th>\n",
    "    <th>Cada 3 movimientos uno aleatorio</th>\n",
    "    <th>Cada 4 movimientos uno aleatorio</th>\n",
    "    <th>Cada 5 movimientos uno aleatorio</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>%</td>\n",
    "    <td>77.7%</td>\n",
    "    <td>86.6%</td>\n",
    "    <td>80.5%</td>\n",
    "    <td>83.3%</td>\n",
    "  </tr>    \n",
    "</table>\n",
    "  \n",
    "  En conclusión, nos quedaremos con la configuración que nos dio un total de 86.6% de partidas ganadas. Es decir, elegir un movimiento aleatorio cada 3 movimientos.\n",
    "  \n",
    "  _**Observación:** Notamos que luego de fijar esta variante casi no tenemos empates._\n",
    "  \n",
    "### 3.1.5 Tasa de aprendizaje μ y enfriamiento\n",
    "A nivel conceptual, a mayor valor de μ, mayor es el ajuste. Fuimos probando con distintos valores de μ en cada entrenamiento, y los resultados fueron los siguientes:\n",
    "    \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>μ</th>\n",
    "    <th>% partidas ganadas</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>μ=0.1</td>\n",
    "    <td>93.3</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>μ=0.01</td>\n",
    "    <td>94.4</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>μ=0.001</td>\n",
    "    <td>95.6</td>\n",
    "  </tr>  \n",
    "</table>\n",
    "\n",
    "En un intento por mejorar aún más el porcentaje probamos con distintas tasas de enfriamiento, pero no notamos cambios notorios en los resultados finales.\n",
    "\n",
    "En conclusión, nos quedarnos con μ fijo igual a 0.001.\n",
    "\n",
    "\n",
    "### 3.1.6 Factor de descuento $\\gamma$\n",
    "Definimos un factor de descuento igual a $(0.9)^k$ con el objetivo de dar una menor valoración a los tableros que estan más cerca del tablero inicial.\n",
    "\n",
    "El porcentaje de partidas ganadas sin el factor de descuento fue de 95.6%.\n",
    "\n",
    "Con el factor de descuento fue de 96.4%, por lo que decidimos conservarlo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Oponente inteligente\n",
    "\n",
    "En esta sección veremos los resultados obtenidos al jugar contra un oponente que es una version anterior del jugador 1.\n",
    "\n",
    "### 3.2.1 Oponente con pesos fijos\n",
    " \n",
    "Primero probamos entrenar contra un jugador 2 que utiliza la funcion objetivo $V_{op}$ parte 3.1 y nucna la actualiza :\n",
    "\n",
    "Aquí notamos que con los hiperparamentros utilizados en la parte 3.2 el jugador 1 tenia un porcentaje de victorias mucho menor, alredodr de un 40% lo que nos llevo a cambiar variables.  \n",
    "- Inicialmente aumentamos la cantidad de partidas jugadas.  \n",
    "- Luego cambiamos la cantidad de movimientos aleatorios del jugador 1 a cero, pues en la parte 3.2 ya exploro             nuevos caminos y esta vez queremos que tome siempre el movimiento optimo con la funcion objetivo conocida         hasta el momento.  \n",
    "- Como notamos que igualmente el porcentaje de partidas ganadas era bajo, decidimos probar con distintos factores de aprendizaje.\n",
    "- Otro fenomeno que notamos fue el aumento de empates que en la seccion anterior habiamos logrado bajar.  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>μ = 0.001</th>\n",
    "    <th>μ = 0.01</th>\n",
    "    <th>μ = 0.1</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>jugador 1 23.4% / juegador 2 22.7% </td>\n",
    "    <td>jugador 1 23.9% / juegador 2 21.8%</td>\n",
    "    <td>jugador 1 49.6% / juegador 2 41.0%</td>  \n",
    "  </tr>\n",
    "     <caption>Esta tabla representa el porcentaje de partidas ganadas de cada jugador  jugador 1/jugador 2 variando μ </caption>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Aqui podemos ver tres graficas donde cada una representa la suma de la cantidad de partidas ganadas para cada jugador en funcion de la cantidad de patidas jugadas. La orimera es con μ=0.001 , la segunda con μ=0.01 y la tercera μ=0.1  \n",
    "  \n",
    "<img src=\"grafica1.jpeg\" width=400 height=400>  \n",
    "\n",
    "\n",
    "<img src=\"grafica2.jpeg\" width=400 height=400>  \n",
    "  \n",
    "\n",
    "<img src=\"grafica3.jpeg\" width=400 height=400>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Oponente actualizado\n",
    "\n",
    "Luego acualizamos la funcion $V_{op}$ que ultiliza el jugador 2 para elegir su proximo movimiento cada 20 partidas:\n",
    "\n",
    "Aqui el porcentaje de partidas ganadas por el jugador 1 es y si vemos la grafica analoga a la parte anterior se ve claramente como el jugador 2 gana mas partidads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Competencia\n",
    "\n",
    "En este punto pusimos a competir dos versiones iguales del jugador 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8d88052198>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot\n",
    "matplotlib.pyplot.plot(range(0,100), [2**-(x/10) for x in range(0,100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Los mejores resultados logicamente se dieron cuendo jugabamos con un opnente aleatorio , dado que la inteligencia de tal era nula. A medida que el oponente se volvia mas inteligente el porcentaje de partidas ganadas decrecio notoriamente. En este punto tambien observamos que con un oponete aleatorio casi no habian empates.\n",
    "- Otro punto a destacar, cuando oponente es aleatorio el echo de que usar enfriamiento en la taza de aprendizaje casi no cambio los resultados, con un μ =0.001 que es bastante pequeño obtuvimos buenos resultados. Lo que significa que si desde un principio limitabamos la variacion de el vector $W$ podiamos converger a valores satisfactorios. En cotraposicion con esto cuando el oponente es una version anterior del jugador 1 necitamos una taza de aprendizaje inicial mas alta. Obtuvimos mejores resultados cambiando μ =0.1 y el enfriamiento en este punto tampoco tuvo grandes variaciones en el resultado final.\n",
    "- Tambien notamos que cuando entrenamos contra un oponente random, en un momento el porcentaje de partidas ganadas se estanca. Esto pudimos solucionarlo haciendo que el jugador 1 cada cierta cantidad de movimientos realize uno aleatorio, permitiendo asi que explore nuevos caminos y no se estanque su aprendizaje. \n",
    "  Lo curioso de esto es que cuando entrenamos contra un oponente inteligente tomar movimintos aleatorios no cambia el porcentaje de partidas ganadas. Atribuimos esto a que el la version anteror ya exploro todos los caminos que puede con los hiperparametros que conocidos.  \n",
    "- Algo que creemos que mejoraria el algoritmo es poder normalizar los atributos y los $w_{i}$ de forma tal de poder logara que la imagen de la funcion $V_{op}$ realemnte se encuentrte en el intervalos $[-1,1]$. Logrando asi poder darle valores conocidos a tableros finales. \n",
    "  Inicialmente creimos conveniente que para tener una idea de que valores $w_{i}$ para comenza a jugar podia ser    utiles, crear historiales de juego que conste de solo un tablero final dado que en principio $V_{train}$ para esos tableros era conocida , pero con el avance de la tare nos dimos cuenta que era una falacia dado que existian tableros intemedios con un valor de $V_{op}$ mayor que 1.  \n",
    "- Tambie podria mejorar el algoritmo cambiar la forma en la que elegimos el proximo movimiento, la forma que nosotros aplicamos si bien nos fue util no creemos que sea la mejor. Una alternativa a esto es dado el conjunto de movimientos posibles para el jugador 1 si suponemos que nuestro oponente se mueve de forma optima (segun la $V_{op}$ conocida hasta el momento) podemoms quedarnos con el movimiento que lleve a el oponente a el tablero menor valorado para el."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
